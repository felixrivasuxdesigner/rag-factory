# Estrategia de Ingesta Ã“ptima para RAG Factory

## ðŸ“Š AnÃ¡lisis de Limitaciones

### Chile BCN (datos.bcn.cl)

**Rate Limits**:
- â“ **DESCONOCIDO** - No documentado pÃºblicamente
- ðŸ” Endpoint SPARQL es servicio pÃºblico del gobierno
- âš ï¸ Probablemente tiene lÃ­mites para prevenir abuso
- ðŸ“ RecomendaciÃ³n: **Asumir conservador: ~100-500 requests/dÃ­a**

**CaracterÃ­sticas de Datos**:
- ðŸ“„ TamaÃ±o promedio: ~900 caracteres por documento
- âš¡ Velocidad: ~5-6 segundos por documento (SPARQL + XML fetch)
- âœ… Manejable para Ollama (no causa timeouts)

**Riesgos**:
- ðŸš« Bloqueo de IP si excedemos lÃ­mites
- â±ï¸ Servicio gubernamental puede ser lento en horas pico
- ðŸ”„ XMLs antiguos pueden fallar o no tener contenido

### USA Congress.gov API

**Rate Limits**:
- âœ… **5,000 requests/dÃ­a** con DEMO_KEY (documentado)
- ðŸ“§ Con API key registrada: **5,000-10,000/dÃ­a** (mÃ¡s estable)
- â±ï¸ Recomiendan no exceder 1 request/segundo

**CaracterÃ­sticas de Datos**:
- ðŸ“„ TamaÃ±o variable: 2,000 - 9,700 caracteres por bill
- âš¡ Velocidad: ~10-15 segundos por documento (API + XML download)
- âš ï¸ Bills largos (>5,000 chars) causan timeouts en Ollama

**Riesgos**:
- ðŸ’¥ Worker timeout con bills muy largos
- ðŸ“Š Algunos bills no tienen texto disponible (solo metadata)
- ðŸ”„ Rate limiting si no respetamos delays

### Ollama (Local - Embeddings)

**Limitaciones**:
- â±ï¸ **Timeout actual**: 60 segundos
- ðŸ’¾ **Memoria**: ~700MB en reposo, picos de 2-3GB con textos largos
- ðŸ”¥ **CPU**: 100% durante generaciÃ³n de embeddings
- âš ï¸ **CrÃ­tico**: Textos >5,000 chars pueden causar timeout o crash

**Performance**:
- âœ… Texto corto (<1,000 chars): ~2-3 segundos
- ðŸŸ¡ Texto medio (1,000-3,000 chars): ~5-10 segundos
- ðŸ”´ Texto largo (>5,000 chars): ~15-60+ segundos (riesgo de timeout)

## ðŸŽ¯ Estrategia Ã“ptima Propuesta

### OpciÃ³n 1: Enfoque Conservador y Confiable â­â­â­â­â­

**Para CHILE BCN**:
```yaml
Frecuencia: Cada 6 horas (4 syncs/dÃ­a)
LÃ­mite por sync: 25 documentos
Total diario: ~100 documentos
Delay entre requests: 3 segundos
Timeout: 90 segundos por documento
```

**JustificaciÃ³n**:
- âœ… Bajo riesgo de rate limiting
- âœ… Documentos pequeÃ±os (~900 chars) â†’ sin timeouts
- âœ… 100 docs/dÃ­a es suficiente para mantenerse actualizado
- âœ… 6 horas permite 4 ventanas diarias

**Para USA CONGRESS**:
```yaml
Frecuencia: Cada 2 horas durante horas hÃ¡biles (8am-8pm = 7 syncs/dÃ­a)
LÃ­mite por sync: 15 documentos
Total diario: ~105 documentos
Delay entre requests: 2 segundos
Timeout: 120 segundos por documento
Pre-filtro: Saltar bills >6,000 caracteres (ingestar solo despuÃ©s con chunking especial)
```

**JustificaciÃ³n**:
- âœ… Bien debajo del lÃ­mite de 5,000/dÃ­a
- âœ… Filtro evita timeouts de Ollama
- âœ… 105 bills/dÃ­a cubre nueva legislaciÃ³n fÃ¡cilmente
- âœ… Solo durante horas hÃ¡biles evita sobrecarga nocturna

### OpciÃ³n 2: Enfoque Agresivo (MÃ¡ximo Throughput) â­â­â­

**Para CHILE BCN**:
```yaml
Frecuencia: Cada 3 horas (8 syncs/dÃ­a)
LÃ­mite por sync: 50 documentos
Total diario: ~400 documentos
```

**Para USA CONGRESS**:
```yaml
Frecuencia: Cada hora (24 syncs/dÃ­a)
LÃ­mite por sync: 100 documentos
Total diario: ~2,400 documentos
```

**Riesgos**:
- âš ï¸ Chile: Posible rate limiting
- âš ï¸ Ollama: Sobrecarga continua
- âš ï¸ Ventiladores constantemente activos

### OpciÃ³n 3: Enfoque HÃ­brido (Recomendado) â­â­â­â­â­

**LÃ³gica**:
1. **Primera ingesta completa**: Hacer una sola vez, lÃ­mite de 500 docs por fuente
2. **Syncs incrementales**: Solo documentos nuevos desde Ãºltima sync
3. **Horarios inteligentes**: Diferentes para cada fuente

**Schedule sugerido**:

```
Chile BCN (conservador):
- Lunes-Viernes: 9am, 3pm, 9pm (3 syncs/dÃ­a)
- SÃ¡bado-Domingo: 12pm (1 sync/dÃ­a)
- LÃ­mite: 30 docs/sync
- Total: ~90 docs/dÃ­a laborable, ~30 docs/dÃ­a fin de semana

USA Congress (agresivo en dÃ­as laborables):
- Lunes-Viernes: 9am, 12pm, 3pm, 6pm, 9pm (5 syncs/dÃ­a)
- SÃ¡bado-Domingo: Solo si hay actividad legislativa
- LÃ­mite: 20 docs/sync
- Total: ~100 docs/dÃ­a laborable
```

## ðŸ› ï¸ Soluciones TÃ©cnicas Necesarias

### 1. Chunking Inteligente para Documentos Largos

**Problema**: USA Congress bills de 9,700 chars causan timeout

**SoluciÃ³n**:
```python
def smart_chunking(document, max_chunk_size=2000):
    """
    Si documento > 5000 chars:
    - Dividir en chunks de 2000 chars con 200 overlap
    - Generar embeddings para cada chunk por separado
    - Procesar en lotes pequeÃ±os (5 chunks a la vez)
    """
    if len(document['content']) > 5000:
        chunks = split_into_chunks(document, size=2000, overlap=200)
        # Procesar chunks en batches
        for batch in batched(chunks, batch_size=5):
            process_batch_with_delay(batch, delay=2)
    else:
        # Documento normal, procesar directamente
        process_document(document)
```

### 2. ConfiguraciÃ³n de Sync Schedules por Source

**Schema update necesario**:
```sql
ALTER TABLE data_sources ADD COLUMN sync_schedule JSONB;

-- Ejemplo de schedule:
{
  "enabled": true,
  "frequency": "custom",
  "schedule": {
    "monday": ["09:00", "15:00", "21:00"],
    "tuesday": ["09:00", "15:00", "21:00"],
    "wednesday": ["09:00", "15:00", "21:00"],
    "thursday": ["09:00", "15:00", "21:00"],
    "friday": ["09:00", "15:00", "21:00"],
    "saturday": ["12:00"],
    "sunday": ["12:00"]
  },
  "max_docs_per_sync": 30,
  "delay_between_requests": 3
}
```

### 3. Worker Timeout y Retry Logic

**Worker updates**:
```python
# Aumentar timeout del worker
worker_timeout = 300  # 5 minutos

# Retry logic para documentos que fallan
@retry(max_attempts=2, backoff=exponential)
def process_document_with_retry(doc):
    try:
        return process_document(doc)
    except TimeoutError:
        # Si es documento largo, intentar con chunking
        if len(doc['content']) > 5000:
            return process_with_smart_chunking(doc)
        raise
```

### 4. Incremental Sync (Solo Nuevos Documentos)

**LÃ³gica**:
```python
def incremental_sync(source_id, last_sync_timestamp):
    """
    - Chile: Filtrar por publishDate > last_sync
    - USA: Filtrar por updateDate > last_sync
    - Evita re-procesar documentos existentes
    - Reduce carga significativamente despuÃ©s de primera ingesta
    """
    if source_type == 'chile_fulltext':
        query_filter = f"?publishDate > '{last_sync_timestamp}'"
    elif source_type == 'congress_api':
        params = {'updateDate[gte]': last_sync_timestamp}

    new_docs = fetch_with_filter(query_filter)
    return new_docs
```

## ðŸ“‹ Plan de ImplementaciÃ³n Prioritario

### Fase 1: Fixes CrÃ­ticos (HOY) âš¡
1. âœ… Aumentar worker timeout a 180 segundos
2. âœ… Implementar smart chunking para docs >5,000 chars
3. âœ… AÃ±adir delay configurable entre requests

### Fase 2: Scheduling Flexible (MAÃ‘ANA) ðŸ“…
4. â¬œ AÃ±adir campo `sync_schedule` a data_sources table
5. â¬œ Crear scheduler service (puede usar APScheduler o cron jobs)
6. â¬œ UI para configurar schedules por source

### Fase 3: Optimizaciones (PRÃ“XIMA SEMANA) ðŸš€
7. â¬œ Implementar incremental sync
8. â¬œ AÃ±adir mÃ©tricas de performance (tiempo/doc, errores, etc.)
9. â¬œ Dashboard de sync status

## ðŸŽ¯ RecomendaciÃ³n Final

**Para empezar HOY**:

1. **Chile BCN**:
   - Configurar manualmente 3 syncs/dÃ­a (9am, 3pm, 9pm)
   - 25-30 documentos por sync
   - Total: ~75-90 docs/dÃ­a

2. **USA Congress**:
   - Configurar manualmente 2 syncs/dÃ­a (10am, 4pm)
   - 15-20 documentos por sync con filtro <6,000 chars
   - Total: ~30-40 docs/dÃ­a

3. **Implementar**:
   - Smart chunking (Fase 1.2)
   - Aumentar worker timeout (Fase 1.1)

**Ventajas**:
- âœ… Evita rate limiting completamente
- âœ… No sobrecarga Ollama
- âœ… ~120 documentos/dÃ­a es suficiente para mantenerse actualizado
- âœ… Permite iterar y mejorar sin romper nada

**DespuÃ©s** de validar que funciona bien por 1 semana, podemos:
- Aumentar frecuencia gradualmente
- Implementar scheduling automÃ¡tico
- AÃ±adir mÃ¡s fuentes de datos

---

Â¿QuÃ© opinas? Â¿Empezamos con la implementaciÃ³n de Fase 1 (fixes crÃ­ticos)?
