[
  {
    "id": "rag_fundamentals",
    "title": "RAG Fundamentals: Retrieval-Augmented Generation Explained",
    "content": "Retrieval-Augmented Generation (RAG) is a powerful AI architecture that combines the strengths of information retrieval systems with large language models (LLMs) to produce more accurate, factual, and contextually relevant responses.\n\nThe RAG Pipeline consists of several key stages:\n\n1. DOCUMENT INGESTION: The first step involves collecting and processing documents from various sources. These can include PDFs, web pages, databases, APIs, or any structured/unstructured data. Documents are cleaned, normalized, and prepared for embedding generation.\n\n2. TEXT CHUNKING: Large documents are split into smaller, manageable chunks. Typical chunk sizes range from 500-1500 characters with 100-200 character overlap between chunks. This overlap ensures that context isn't lost at chunk boundaries. The chunking strategy significantly impacts retrieval quality - semantic chunking (splitting at natural boundaries like paragraphs or sections) often outperforms fixed-size chunking.\n\n3. EMBEDDING GENERATION: Each chunk is converted into a dense vector representation (embedding) using an embedding model. Popular models include OpenAI's text-embedding-ada-002, Cohere's embed-multilingual, and open-source options like mxbai-embed-large or BGE. These embeddings capture the semantic meaning of text in high-dimensional space (typically 768-1536 dimensions), where similar concepts are positioned close together.\n\n4. VECTOR STORAGE: Embeddings are stored in a specialized vector database optimized for similarity search. Options include Pinecone (managed cloud), Weaviate (open-source), Milvus (scalable), Qdrant (Rust-based), Chroma (embedded), and pgvector (PostgreSQL extension). These databases use indexing structures like HNSW or IVF to enable fast approximate nearest neighbor search.\n\n5. QUERY PROCESSING: When a user asks a question, it's converted into an embedding using the same embedding model used for documents. This ensures consistency in the vector space.\n\n6. SIMILARITY SEARCH: The query embedding is compared against stored document embeddings using distance metrics like cosine similarity, Euclidean distance, or dot product. The top-k most similar chunks are retrieved (typically k=3-10).\n\n7. CONTEXT INJECTION: Retrieved chunks are formatted and injected into the LLM prompt as context. A typical prompt structure includes: system instructions, retrieved context, and the user question.\n\n8. RESPONSE GENERATION: The LLM generates an answer grounded in the provided context. Modern approaches use techniques like chain-of-thought prompting, instructing the model to cite sources, and constraining responses to only use provided information.\n\nBenefits of RAG:\n- Reduces hallucinations by grounding responses in factual data\n- Enables up-to-date information without retraining the LLM\n- Allows domain-specific knowledge without fine-tuning\n- Provides source attribution and transparency\n- Cost-effective compared to training custom models\n- Works with any LLM (GPT-4, Claude, Gemma, Llama, etc.)\n\nChallenges:\n- Chunking strategy affects retrieval quality\n- Requires careful prompt engineering\n- Embedding model quality impacts results\n- Context window limitations in LLMs\n- Need for deduplication and freshness management"
  },
  {
    "id": "vector_databases",
    "title": "Vector Databases: Architecture and Use Cases",
    "content": "Vector databases are specialized systems designed to efficiently store, index, and query high-dimensional vector embeddings. Unlike traditional databases that organize data in rows and tables, vector databases use specialized data structures optimized for similarity search in multi-dimensional space.\n\nCORE CONCEPTS:\n\nVector Embeddings: Numerical representations of data (text, images, audio, video) in high-dimensional space. A 768-dimensional embedding is a list of 768 floating-point numbers that captures semantic meaning. Similar items have similar embeddings - for example, 'king' and 'queen' would be close together in embedding space.\n\nSimilarity Metrics determine how we measure distance between vectors:\n\n- Cosine Similarity: Measures the cosine of the angle between vectors. Range: -1 to 1 (1 = identical direction). Best for text embeddings where magnitude is less important than direction. Formula: cos(θ) = (A·B) / (||A|| ||B||)\n\n- Euclidean Distance: Straight-line distance between points. Sensitive to magnitude. Formula: d = sqrt(Σ(ai - bi)²)\n\n- Dot Product: Measures both magnitude and direction. Faster to compute but not normalized. Formula: A·B = Σ(ai × bi)\n\n- Manhattan Distance: Sum of absolute differences. Used when dimensions have different units.\n\nINDEXING METHODS:\n\nVector databases use specialized indexes to avoid brute-force search of all vectors:\n\n1. HNSW (Hierarchical Navigable Small World): Graph-based index that builds a multi-layer graph structure. Provides excellent query performance (sub-millisecond) with good recall. Memory-intensive but very fast. Used by: Qdrant, Weaviate, Milvus.\n\n2. IVF (Inverted File Index): Clustering-based approach. Vectors are partitioned into clusters using k-means. Search only examines vectors in nearby clusters. Good balance of speed and memory. Used by: FAISS, Milvus.\n\n3. LSH (Locality-Sensitive Hashing): Hash functions that map similar vectors to same buckets. Probabilistic, very fast, lower accuracy. Good for extremely large datasets.\n\n4. Product Quantization: Compresses vectors by splitting dimensions and quantizing. Reduces memory usage 10-100x with small accuracy loss.\n\nPOPULAR VECTOR DATABASES:\n\n- Pinecone: Fully managed, cloud-native. Easy to use, automatic scaling. Proprietary. Best for: Production applications, teams wanting managed solution.\n\n- Weaviate: Open-source, GraphQL API. Built-in vectorization, hybrid search. Best for: Complex queries, knowledge graphs.\n\n- Milvus: Open-source, highly scalable. Supports multiple indexes, distributed architecture. Best for: Large-scale deployments, billions of vectors.\n\n- Qdrant: Rust-based, fast, modern API. Payload filtering, good for filtering+vector search. Best for: Performance-critical applications.\n\n- Chroma: Embedded, developer-friendly. SQLite-like simplicity. Best for: Prototyping, small applications.\n\n- pgvector: PostgreSQL extension. Familiar SQL interface, ACID compliance. Best for: Existing PostgreSQL users, transactional consistency.\n\nUSE CASES:\n\n1. Semantic Search: Find documents by meaning, not keywords. Example: Search 'how to cook pasta' matches 'boiling noodles recipe'.\n\n2. Recommendation Systems: Find similar products, content, users based on embeddings.\n\n3. RAG Systems: Retrieve relevant context for LLM responses.\n\n4. Image Similarity: Find visually similar images using vision embeddings.\n\n5. Anomaly Detection: Identify outliers by finding vectors far from clusters.\n\n6. Duplicate Detection: Find near-duplicate content by similarity threshold.\n\n7. Question Answering: Match questions to similar historical Q&A pairs.\n\nPERFORMANCE CONSIDERATIONS:\n\n- Index build time increases with dataset size\n- Query latency: typically 1-100ms depending on dataset and accuracy requirements\n- Memory usage: full-precision vectors need 4 bytes × dimensions × count\n- Accuracy vs. Speed tradeoff: exact search is slow, approximate is fast\n- Batch ingestion is much faster than single inserts"
  },
  {
    "id": "llm_prompting",
    "title": "Advanced LLM Prompt Engineering Techniques",
    "content": "Prompt engineering is the art and science of crafting effective inputs for Large Language Models to achieve desired outputs. As LLMs become more powerful, prompt engineering has evolved from simple instructions to sophisticated techniques that unlock their full potential.\n\nFUNDAMENTAL PRINCIPLES:\n\n1. BE SPECIFIC AND CLEAR: Vague prompts yield vague results. Instead of 'Tell me about Python', use 'Explain the difference between Python lists and tuples with code examples, focusing on mutability and performance implications'.\n\n2. PROVIDE CONTEXT: Give the model relevant background information. Context can include: role definition ('You are an expert data scientist'), domain knowledge, constraints, and audience level.\n\n3. STRUCTURE YOUR PROMPTS: Use clear sections with markdown formatting:\n```\n## Task\nAnalyze this code for security vulnerabilities\n\n## Code\n[code here]\n\n## Focus Areas\n- SQL injection\n- XSS\n- Authentication bypass\n\n## Output Format\nJSON with severity levels\n```\n\nADVANCED TECHNIQUES:\n\nFEW-SHOT LEARNING:\nProvide examples of desired input-output pairs before the actual task. This 'teaches' the model the pattern:\n\nExample:\n```\nInput: The movie was fantastic!\nOutput: Positive\n\nInput: Terrible experience, would not recommend.\nOutput: Negative\n\nInput: It was okay, nothing special.\nOutput: Neutral\n\nInput: [your text here]\nOutput:\n```\n\nCHAIN OF THOUGHT (CoT):\nAsk the model to think step-by-step. This dramatically improves reasoning on complex problems:\n\n'Let's solve this step by step:\n1) First, identify the key information\n2) Then, apply the relevant formula\n3) Finally, verify the answer'\n\nZERO-SHOT CoT:\nSimply adding 'Let's think step by step' before the question improves performance on reasoning tasks.\n\nTREE OF THOUGHTS:\nExplore multiple reasoning paths simultaneously:\n'Consider three different approaches to solve this problem. For each approach, evaluate pros and cons. Then choose the best one.'\n\nREACT (Reasoning + Acting):\nCombine reasoning with actions in tools:\n'Thought: I need to find current stock prices\nAction: search_stocks(\"AAPL\")\nObservation: [result]\nThought: Now I'll analyze the data...'\n\nSELF-CONSISTENCY:\nGenerate multiple answers and take the majority vote. Improves accuracy on reasoning tasks.\n\nPROMPT CHAINING:\nBreak complex tasks into steps, using output from one prompt as input to the next:\nStep 1: Extract key entities\nStep 2: Classify each entity  \nStep 3: Generate summary\n\nCONSTRAINING OUTPUTS:\n\n- Format constraints: 'Respond in JSON', 'Use bullet points', 'Maximum 3 sentences'\n- Content constraints: 'Only use information from the provided context', 'Do not mention competitors'\n- Tone constraints: 'Write in a professional tone', 'Use simple language for beginners'\n\nPARAMETER TUNING:\n\nTemperature (0.0 - 2.0):\n- 0.0-0.3: Deterministic, focused, factual. Use for: code generation, fact extraction, classification\n- 0.4-0.7: Balanced creativity and consistency. Use for: general Q&A, summarization\n- 0.8-1.0: Creative, diverse outputs. Use for: brainstorming, creative writing\n- 1.1-2.0: Very random, experimental. Rarely useful in production\n\ntop_p (0.0 - 1.0): Nucleus sampling\n- 0.9-1.0: Consider most tokens (diverse)\n- 0.1-0.5: Consider only high-probability tokens (focused)\n\nfrequency_penalty: Reduce repetition\npresence_penalty: Encourage new topics\nmax_tokens: Limit response length\n\nVALIDATION AND GROUNDING:\n\nFor RAG applications:\n```\nYou are a helpful assistant that answers questions based ONLY on the provided context.\n\nRules:\n1. If the answer is not in the context, say 'I don't have enough information'\n2. Cite specific parts of the context\n3. Do not use external knowledge\n4. If uncertain, express the uncertainty\n\nContext:\n[retrieved documents]\n\nQuestion:\n[user question]\n\nAnswer:\n```\n\nCOMMON PITFALLS:\n\n- Assuming the model knows recent events (has knowledge cutoff)\n- Not handling edge cases in prompts\n- Over-engineering prompts (simpler often works better)\n- Not testing prompts with diverse inputs\n- Ignoring cost implications of long prompts\n\nBEST PRACTICES:\n\n1. Iterate: Start simple, test, refine based on outputs\n2. Version control: Track prompt changes and performance\n3. A/B test: Compare different prompt variations\n4. Monitor: Track failure cases and hallucinations\n5. Cache: Reuse embeddings and frequent responses\n6. Validate: Always verify factual claims\n7. Fallback: Have graceful degradation for failures"
  },
  {
    "id": "embeddings_guide",
    "title": "Understanding Text Embeddings: From Words to Vectors",
    "content": "Text embeddings are numerical representations that capture the semantic meaning of words, sentences, or documents in a continuous vector space. They are the foundation of modern NLP and essential for RAG systems, semantic search, and many AI applications.\n\nHISTORICAL EVOLUTION:\n\nWord2Vec (2013): Revolutionary approach using shallow neural networks. Introduced two architectures:\n- CBOW (Continuous Bag of Words): Predicts word from context\n- Skip-gram: Predicts context from word\nKey insight: Words used in similar contexts have similar meanings. Captures analogies like 'king - man + woman = queen'. Limited to ~300 dimensions, single vector per word regardless of context.\n\nGloVe (2014): Global Vectors for word representation. Uses word co-occurrence statistics from corpus. Combines benefits of global matrix factorization and local context windows. Faster training than Word2Vec.\n\nFastText (2016): Extension of Word2Vec that represents words as bag of character n-grams. Handles out-of-vocabulary words and morphological variations. Great for morphologically rich languages.\n\nELMo (2018): Embeddings from Language Models. First contextual embeddings - same word gets different vectors based on context. 'bank' in 'river bank' vs 'financial bank' have different embeddings. BiLSTM architecture, ~1024 dimensions.\n\nBERT (2018): Bidirectional Encoder Representations from Transformers. Breakthrough in contextual embeddings using transformer architecture. Pre-trained on massive corpora, fine-tunable for specific tasks. 768 dimensions (base) or 1024 (large).\n\nMODERN EMBEDDING MODELS (2020s):\n\nSentence Transformers: Specialized for sentence-level embeddings. Uses siamese networks to learn good sentence representations. Popular models:\n- all-MiniLM-L6-v2: Fast, 384 dimensions, good general purpose\n- all-mpnet-base-v2: Higher quality, 768 dimensions\n- multi-qa-mpnet-base: Optimized for Q&A\n\nOpenAI Embeddings:\n- text-embedding-ada-002: 1536 dimensions, excellent quality, API-based\n- Used by ChatGPT, strong semantic understanding\n- Cost: $0.0001 per 1K tokens\n\nCohere Embeddings:\n- embed-english-v3.0: 1024 dimensions\n- embed-multilingual-v3.0: Supports 100+ languages\n- Compression options for efficiency\n\nOpen-Source Leaders:\n- BGE (BAAI General Embedding): State-of-art open-source, trained on massive data\n- E5: Microsoft's embedding family, strong performance\n- mxbai-embed-large: 1024 dimensions, optimized for RAG, used by Ollama\n\nEMBEDDING PROPERTIES:\n\nDimensionality:\n- Higher dimensions = more information capacity\n- But: higher memory, slower similarity search, potential overfitting\n- Sweet spot: 768-1536 for most applications\n- Dimensionality reduction (PCA, UMAP) possible but loses information\n\nSemantic Similarity:\nEmbeddings capture meaning, not just word overlap:\n- 'automobile' and 'car' are very similar\n- 'bank' (financial) and 'bank' (river) are different\n- 'The cat sat on the mat' and 'A feline rested on a rug' are similar\n\nClustering:\nSimilar concepts form clusters in embedding space:\n- Animals cluster together\n- Programming languages cluster together\n- Emotions cluster together\n\nAnalogies:\nVector arithmetic captures relationships:\n- king - man + woman ≈ queen\n- Paris - France + Italy ≈ Rome\n- walking - walk + swim ≈ swimming\n\nCHOOSING AN EMBEDDING MODEL:\n\nConsider:\n\n1. Domain: General-purpose vs. specialized (medical, legal, code)\n2. Languages: Multilingual support needed?\n3. Performance: Quality vs. speed tradeoff\n4. Dimensions: Memory and compute constraints\n5. License: Open-source vs. commercial\n6. Deployment: API vs. self-hosted\n7. Cost: Inference pricing for API models\n\nFor RAG systems:\n- Match: Use same model for documents and queries\n- Quality: Prioritize semantic understanding over speed\n- Context: Prefer models trained on similar domains\n- Length: Ensure model handles your text lengths (some max at 512 tokens)\n\nBEST PRACTICES:\n\n1. Normalize: Some models require L2 normalization\n2. Prefix: Some models benefit from task-specific prefixes ('query: ' vs 'passage: ')\n3. Batch: Generate embeddings in batches for efficiency\n4. Cache: Store embeddings, don't regenerate\n5. Version: Track which embedding model version you used\n6. Test: Evaluate on your specific data, benchmarks don't tell the whole story\n7. Update: Re-embed documents when upgrading embedding models\n\nCOMMON ISSUES:\n\n- Out-of-vocabulary: Words model hasn't seen (use subword models like FastText)\n- Context window: Text longer than model's max length gets truncated\n- Domain shift: Model trained on Wikipedia struggles with medical jargon\n- Multilingual: Single-language models fail on mixed-language text\n- Computational cost: Large models are slow, consider model compression\n\nEVALUATION METRICS:\n\n- Cosine similarity: Primary metric for semantic similarity\n- Hit rate@k: Percentage of queries where relevant doc is in top-k\n- MRR (Mean Reciprocal Rank): Average of 1/rank of first relevant result\n- NDCG (Normalized Discounted Cumulative Gain): Ranking quality metric\n- Embedding quality: Measured on benchmark datasets (MTEB, BEIR)"
  },
  {
    "id": "rag_production",
    "title": "Production RAG Systems: Best Practices and Pitfalls",
    "content": "Deploying RAG (Retrieval-Augmented Generation) systems in production environments requires careful consideration of architecture, performance, monitoring, and maintenance. This guide covers battle-tested practices from real-world deployments.\n\nARCHITECTURE PATTERNS:\n\n1. BASIC RAG (Naive RAG):\nUser Query → Embed → Vector Search → Top-K Docs → LLM → Response\n\nPros: Simple, fast to implement\nCons: Limited context window, no query optimization, basic retrieval\nUse case: Simple Q&A, small document sets\n\n2. ADVANCED RAG:\nQuery Rewriting → Multi-Stage Retrieval → Re-ranking → Context Compression → LLM → Response + Citations\n\nComponents:\n- Query expansion: Generate variations of user query\n- Hypothetical Document Embeddings (HyDE): Generate hypothetical answer, embed it, search with that\n- Multi-query: Break complex query into sub-queries\n- Re-ranking: Use cross-encoder to re-score retrieved docs (more accurate but slower)\n- Context compression: Summarize or extract key sentences from retrieved chunks\n\n3. MODULAR RAG:\nSeparate concerns into microservices:\n- Ingestion Service: Document processing, chunking, embedding\n- Index Service: Vector database management\n- Retrieval Service: Query processing, search\n- Generation Service: LLM inference\n- Orchestration Service: Coordinates components\n\nBenefits: Independent scaling, easier testing, technology flexibility\n\nDATA MANAGEMENT:\n\nChunking Strategies:\n\n- Fixed-size: Simple, consistent, but may split important context\n- Semantic: Split at paragraph/section boundaries, preserves context, variable sizes\n- Sliding window: Overlapping chunks, ensures no context loss, more storage\n- Recursive: Hierarchical splitting (document → sections → paragraphs → sentences)\n\nOptimal chunk size depends on:\n- Embedding model context window\n- LLM context window  \n- Document structure (technical docs vs. narrative)\n- Query granularity (specific facts vs. broad topics)\n\nRule of thumb: 500-1000 characters with 100-200 overlap\n\nMetadata Management:\n\nEnrich chunks with metadata for filtering:\n```json\n{\n  \"chunk_id\": \"doc123_chunk_5\",\n  \"content\": \"...\",\n  \"metadata\": {\n    \"source\": \"technical_manual.pdf\",\n    \"page\": 42,\n    \"section\": \"Installation Guide\",\n    \"date_published\": \"2024-01-15\",\n    \"author\": \"Engineering Team\",\n    \"tags\": [\"installation\", \"linux\", \"docker\"],\n    \"security_level\": \"public\"\n  }\n}\n```\n\nUse metadata for:\n- Access control (security_level)\n- Temporal filtering (date ranges)\n- Source attribution (citations)\n- Domain filtering (by tag, section)\n\nHYBRID SEARCH:\n\nCombine vector search with traditional keyword search:\n\n1. Vector search (semantic): Understands meaning, handles synonyms\n2. Keyword search (BM25): Exact matches, rare terms, names\n3. Fusion: Combine scores with weighted average or reciprocal rank fusion\n\nWhen to use:\n- Technical terms, product names (keyword excels)\n- Conceptual queries (vector excels)\n- Best of both: hybrid\n\nQUERY OPTIMIZATION:\n\nQuery Expansion:\n```\nOriginal: \"reduce docker image size\"\nExpanded: [\n  \"reduce docker image size\",\n  \"minimize container image footprint\",\n  \"docker optimization techniques\",\n  \"smaller docker builds\"\n]\n```\n\nQuery Classification:\nRoute different query types to specialized handlers:\n- Factual: Direct retrieval\n- Analytical: Multi-step reasoning\n- Navigational: Document structure search\n- Conversational: Dialog state management\n\nPERFORMANCE OPTIMIZATION:\n\nLatency Budget (typical):\n- Embedding generation: 50-200ms\n- Vector search: 10-100ms\n- Re-ranking (optional): 100-500ms\n- LLM generation: 1000-5000ms\n- Total: ~2-6 seconds\n\nOptimization strategies:\n\n1. Caching:\n   - Query cache: Hash query → cached response (same question)\n   - Embedding cache: Store frequently used query embeddings\n   - LLM cache: Cache responses for common queries\n\n2. Async processing:\n   - Parallel retrieval from multiple indexes\n   - Background embedding generation for new docs\n   - Streaming LLM responses (perceived performance)\n\n3. Batch operations:\n   - Batch embed multiple queries\n   - Batch vector search\n   - Batch re-ranking\n\n4. Index optimization:\n   - Use approximate search (HNSW, IVF)\n   - Reduce vector dimensions (PCA, quantization)\n   - Shard large indexes\n\nQUALITY ASSURANCE:\n\nRetrieval Metrics:\n- Precision@k: Relevant docs in top-k / k\n- Recall@k: Relevant docs in top-k / total relevant\n- MRR (Mean Reciprocal Rank): 1/rank of first relevant\n\nGeneration Metrics:\n- Groundedness: Answer uses only provided context\n- Relevance: Answer addresses the question\n- Coherence: Response is well-structured\n- Faithfulness: No hallucinations\n\nEvaluation approaches:\n- Human evaluation (gold standard, expensive)\n- LLM-as-judge (GPT-4 evaluates responses, scalable)\n- Automated tests (unit tests for known Q&A pairs)\n- User feedback (thumbs up/down, captures real quality)\n\nMONITORING:\n\nKey metrics to track:\n\n1. System:\n   - Query latency (p50, p95, p99)\n   - Throughput (queries/second)\n   - Error rate\n   - Resource utilization (CPU, memory, GPU)\n\n2. Quality:\n   - Retrieval quality (precision, recall)\n   - User satisfaction (feedback scores)\n   - Fallback rate (queries with no good retrieval)\n   - Citation accuracy\n\n3. Business:\n   - Query volume trends\n   - Popular topics/questions\n   - User engagement (follow-up questions)\n   - Cost per query\n\nCOMMON PITFALLS:\n\n1. Chunk size mismatch: Chunks too large (exceed LLM context) or too small (lack context)\n2. Stale data: Not updating embeddings when documents change\n3. No access control: Retrieving documents user shouldn't see\n4. Over-retrieval: Passing too many irrelevant chunks to LLM\n5. Under-retrieval: Missing relevant information\n6. Ignoring citations: Users can't verify sources\n7. No fallback: System fails ungracefully when retrieval fails\n8. Cost explosion: Not monitoring LLM API costs\n9. Prompt injection: User queries manipulate system prompts\n10. No versioning: Can't reproduce results after model updates\n\nSECURITY:\n\n- Input validation: Sanitize user queries\n- Access control: Filter retrieved docs by user permissions\n- Rate limiting: Prevent abuse\n- PII handling: Redact sensitive information\n- Audit logging: Track what was retrieved and shown\n- Prompt injection defense: Isolate user input from system prompts"
  },
  {
    "id": "chunking_strategies",
    "title": "Text Chunking Strategies for Optimal RAG Performance",
    "content": "Text chunking is one of the most critical yet often overlooked aspects of RAG systems. The way you split documents directly impacts retrieval quality, context coherence, and ultimately the accuracy of generated responses. This guide explores various chunking strategies and when to use them.\n\nWHY CHUNKING MATTERS:\n\nEmbedding models and LLMs have context window limitations:\n- Embedding models: typically 512-8192 tokens\n- LLMs: 4K-128K tokens (but quality degrades with long contexts)\n- Cost: More tokens = higher API costs\n- Focus: Smaller chunks = more precise retrieval, but may lack context\n\nThe chunking strategy determines:\n1. What gets retrieved (granularity)\n2. How much context is available (coherence)\n3. System performance (chunk count affects search speed)\n4. Storage costs (more chunks = more embeddings)\n\nCHUNKING STRATEGIES:\n\n1. FIXED-SIZE CHUNKING:\n\nSplit text into fixed character or token counts:\n\n```python\nchunk_size = 1000  # characters\noverlap = 200      # characters\n\nchunks = []\nstart = 0\nwhile start < len(text):\n    end = start + chunk_size\n    chunk = text[start:end]\n    chunks.append(chunk)\n    start = end - overlap  # overlap prevents context loss\n```\n\nPros:\n- Simple to implement\n- Predictable chunk sizes\n- Consistent embedding generation time\n- Works with any text\n\nCons:\n- Splits mid-sentence or mid-paragraph\n- Breaks semantic units\n- No respect for document structure\n\nBest for: Unstructured text, chat logs, social media\n\n2. SEMANTIC CHUNKING:\n\nSplit at natural boundaries (sentences, paragraphs, sections):\n\n```python\nimport nltk\n\n# Split into sentences\nsentences = nltk.sent_tokenize(text)\n\n# Group sentences into chunks\nchunks = []\ncurrent_chunk = []\ncurrent_length = 0\n\nfor sentence in sentences:\n    if current_length + len(sentence) > max_chunk_size:\n        chunks.append(' '.join(current_chunk))\n        current_chunk = [sentence]\n        current_length = len(sentence)\n    else:\n        current_chunk.append(sentence)\n        current_length += len(sentence)\n```\n\nPros:\n- Preserves semantic coherence\n- Chunks are meaningful units\n- Better for question answering\n\nCons:\n- Variable chunk sizes\n- More complex implementation\n- Requires language-specific sentence detection\n\nBest for: Articles, documentation, books\n\n3. RECURSIVE CHUNKING:\n\nHierarchical splitting (document → sections → paragraphs → sentences):\n\n```python\ndef recursive_chunk(text, max_size):\n    if len(text) <= max_size:\n        return [text]\n    \n    # Try splitting by paragraphs\n    paragraphs = text.split('\\n\\n')\n    if len(paragraphs) > 1:\n        chunks = []\n        for para in paragraphs:\n            chunks.extend(recursive_chunk(para, max_size))\n        return chunks\n    \n    # Try splitting by sentences\n    sentences = nltk.sent_tokenize(text)\n    if len(sentences) > 1:\n        chunks = []\n        for sentence in sentences:\n            chunks.extend(recursive_chunk(sentence, max_size))\n        return chunks\n    \n    # Force split if still too large\n    return [text[i:i+max_size] for i in range(0, len(text), max_size)]\n```\n\nPros:\n- Respects document structure\n- Graceful degradation\n- Preserves context at multiple levels\n\nCons:\n- Most complex to implement\n- Can produce very different chunk sizes\n\nBest for: Structured documents, technical manuals, legal texts\n\n4. DOCUMENT-STRUCTURE-BASED:\n\nUse explicit document structure (HTML tags, Markdown headers, XML):\n\n```python\nimport markdown\nfrom bs4 import BeautifulSoup\n\ndef chunk_by_headers(markdown_text):\n    html = markdown.markdown(markdown_text)\n    soup = BeautifulSoup(html, 'html.parser')\n    \n    chunks = []\n    current_chunk = {'header': '', 'content': []}\n    \n    for element in soup.find_all(['h1', 'h2', 'h3', 'p']):\n        if element.name in ['h1', 'h2', 'h3']:\n            if current_chunk['content']:\n                chunks.append(current_chunk)\n            current_chunk = {\n                'header': element.text,\n                'content': []\n            }\n        else:\n            current_chunk['content'].append(element.text)\n    \n    if current_chunk['content']:\n        chunks.append(current_chunk)\n    \n    return chunks\n```\n\nPros:\n- Semantic units match document structure\n- Preserves hierarchy\n- Easy to attribute sources\n\nCons:\n- Requires structured input\n- Variable chunk sizes\n\nBest for: Technical docs, wikis, websites\n\n5. SLIDING WINDOW:\n\nOverlapping fixed-size windows:\n\n```python\ndef sliding_window_chunks(text, window_size=1000, stride=800):\n    chunks = []\n    start = 0\n    while start < len(text):\n        end = min(start + window_size, len(text))\n        chunks.append(text[start:end])\n        start += stride  # stride < window_size creates overlap\n    return chunks\n```\n\nPros:\n- Ensures no context is lost\n- Good for finding information that spans boundaries\n- Robust to edge cases\n\nCons:\n- More storage (more chunks)\n- Potential duplicate information in results\n- More embeddings to generate and search\n\nBest for: Cases where missing information is costly\n\nADVANCED TECHNIQUES:\n\nEMBEDDING-BASED CHUNKING:\n\nUse embedding similarity to determine chunk boundaries:\n\n```python\ndef semantic_split(text, embedding_model, similarity_threshold=0.8):\n    sentences = nltk.sent_tokenize(text)\n    embeddings = embedding_model.encode(sentences)\n    \n    chunks = []\n    current_chunk = [sentences[0]]\n    \n    for i in range(1, len(sentences)):\n        # Check similarity with previous sentence\n        similarity = cosine_similarity(\n            embeddings[i-1], \n            embeddings[i]\n        )\n        \n        if similarity < similarity_threshold:\n            # Topic shift, start new chunk\n            chunks.append(' '.join(current_chunk))\n            current_chunk = [sentences[i]]\n        else:\n            current_chunk.append(sentences[i])\n    \n    if current_chunk:\n        chunks.append(' '.join(current_chunk))\n    \n    return chunks\n```\n\nPros:\n- Automatically detects topic shifts\n- Semantically coherent chunks\n\nCons:\n- Computationally expensive\n- Requires embedding model\n- Needs tuning of threshold\n\nCONTEXT ENHANCEMENT:\n\nAdd metadata or surrounding context to chunks:\n\n```python\ndef enhanced_chunks(document):\n    chunks = chunk_text(document['content'])\n    \n    enhanced = []\n    for i, chunk in enumerate(chunks):\n        # Add document metadata\n        enhanced_chunk = f\"\"\"\nDocument: {document['title']}\nSection: {document['section']}\nChunk {i+1}/{len(chunks)}\n\n{chunk}\n\"\"\"\n        enhanced.append(enhanced_chunk)\n    \n    return enhanced\n```\n\nBenefits:\n- Better retrieval (metadata helps matching)\n- Context for LLM (knows where chunk came from)\n- Source attribution\n\nPARENT-CHILD CHUNKING:\n\nStore small chunks for retrieval, but pass larger context to LLM:\n\n```python\nparent_chunk = \"[large section of text]\"\nchild_chunks = [\n    \"chunk 1\",\n    \"chunk 2\", \n    \"chunk 3\"\n]\n\n# Store child chunks with reference to parent\nfor child in child_chunks:\n    store_embedding(child, parent_reference=parent_chunk)\n\n# At query time:\nretrieved_children = search(query, top_k=5)\nparent_chunks = [get_parent(child) for child in retrieved_children]\nllm_context = unique(parent_chunks)  # Pass full context to LLM\n```\n\nBenefits:\n- Precise retrieval (small chunks)\n- Rich context for generation (large chunks)\n- Best of both worlds\n\nCHOOSING A STRATEGY:\n\nConsider:\n\n1. Document Type:\n   - Structured (docs, manuals): Document-structure-based\n   - Unstructured (chat, emails): Fixed-size or Semantic\n   - Mixed: Recursive\n\n2. Query Nature:\n   - Specific facts: Smaller chunks (200-400 chars)\n   - Conceptual understanding: Larger chunks (800-1500 chars)\n   - Mixed: Medium chunks (500-800 chars)\n\n3. Performance:\n   - Many docs, fast search: Fixed-size (predictable)\n   - Quality over speed: Semantic or Recursive\n\n4. Context Window:\n   - LLM context limited: Smaller chunks, more focused retrieval\n   - Large context (GPT-4 128K): Can use larger chunks\n\nBEST PRACTICES:\n\n1. Overlap: Use 10-20% overlap to prevent context loss\n2. Metadata: Always include source, position, date\n3. Test: Evaluate on your specific use case\n4. Monitor: Track which chunks get retrieved, which don't\n5. Iterate: Adjust based on user feedback and metrics\n6. Version: Track chunking strategy version\n7. Re-chunk: When strategy changes, re-process all documents"
  }
]
